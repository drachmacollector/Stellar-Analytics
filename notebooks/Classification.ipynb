{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71f76833"
      },
      "source": [
        "# Classification for KOI Exoplanets\n",
        "Load the \"supernova_dataset.csv\" file, preprocess the data by removing 'CANDIDATE' rows, mapping 'CONFIRMED' to 1 and 'FALSE POSITIVE' to 0, and dropping specified ID and post-analysis leakage columns. Develop and train multiple classification models (SVM, RandomForest, AdaBoost, MLPClassifier) using `GridSearchCV` with `StratifiedKFold`, `f1` scoring, and `roc_auc` computation, incorporating data preprocessing (imputation, scaling) and `PCA` within `sklearn.pipeline.Pipeline` objects. Select the best performing model based on F1-score, its variance, and ROC-AUC. Retrain the chosen best model and its entire pipeline on the full dataset, then save the complete pipeline as \"classification_pipeline.pkl\". Finally, present the metrics of the best model and the complete, well-commented Python code for the classification system, ensuring it is ready for deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f00a2f1"
      },
      "source": [
        "## Load and Initial Preprocess Data\n",
        "\n",
        "### Subtask:\n",
        "Load the 'supernova_dataset.csv' into a pandas DataFrame. Remove rows where 'koi_disposition' is 'CANDIDATE'. Map 'CONFIRMED' to 1 and 'FALSE POSITIVE' to 0 in the 'koi_disposition' column. Identify and drop specified ID and post-analysis leakage columns (e.g., kepler_name, koi_tce_delivname, koi_tce_plnt_num, and target-derived fields). Separate features (X) and target (y) for modeling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53f0acec"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires loading data, filtering, mapping values, dropping specific columns, and separating features and target. I will combine these operations into a single Python code block for efficiency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d83bae2",
        "outputId": "612a78e8-3993-4c70-c77d-edbd3fc3d5f8"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/supernova_dataset.csv')\n",
        "\n",
        "# Remove rows where 'koi_disposition' is 'CANDIDATE'\n",
        "df = df[df['koi_disposition'] != 'CANDIDATE']\n",
        "\n",
        "# Map 'CONFIRMED' to 1 and 'FALSE POSITIVE' to 0\n",
        "df['koi_disposition'] = df['koi_disposition'].map({'CONFIRMED': 1, 'FALSE POSITIVE': 0})\n",
        "\n",
        "# Identify and drop specified ID and post-analysis leakage columns\n",
        "columns_to_drop = [\n",
        "    'kepler_name', 'koi_tce_delivname', 'koi_tce_plnt_num', 'kepid', 'kepoi_name',\n",
        "    'koi_score', 'koi_fpflag_nt', 'koi_fpflag_ss', 'koi_fpflag_co', 'koi_fpflag_ec', 'koi_ror', 'radius_err1', 'radius_err2', 'st_dens'\n",
        "]\n",
        "\n",
        "# Dynamically find and append all error columns to the drop list\n",
        "error_cols = [col for col in df.columns if 'err1' in col or 'err2' in col]\n",
        "columns_to_drop.extend(error_cols)\n",
        "\n",
        "# Drop columns that exist in the DataFrame\n",
        "existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
        "df = df.drop(columns=existing_columns_to_drop)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop(columns=['koi_disposition', 'koi_prad'])\n",
        "y = df['koi_disposition']\n",
        "\n",
        "print(\"Data loading and preprocessing complete.\")\n",
        "print(\"Shape of features (X):\", X.shape)\n",
        "print(\"Shape of target (y):\", y.shape)\n",
        "print(\"First 5 rows of X:\\n\", X.head())\n",
        "print(\"First 5 rows of y:\\n\", y.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loading and preprocessing complete.\n",
            "Shape of features (X): (7585, 11)\n",
            "Shape of target (y): (7585,)\n",
            "First 5 rows of X:\n",
            "    koi_period  koi_duration  koi_depth  koi_impact  koi_model_snr  \\\n",
            "0    9.488036       2.95750      615.8       0.146           35.8   \n",
            "1   54.418383       4.50700      874.8       0.586           25.8   \n",
            "3    1.736952       2.40641     8079.2       1.276          505.6   \n",
            "4    2.525592       1.65450      603.3       0.701           40.9   \n",
            "5   11.094321       4.59450     1517.5       0.538           66.5   \n",
            "\n",
            "   koi_num_transits  st_teff  st_logg  st_met  st_mass  st_radius  \n",
            "0             142.0   5762.0    4.426    0.14   0.9850      0.989  \n",
            "1              25.0   5762.0    4.426    0.14   0.9850      0.989  \n",
            "3             621.0   5805.0    4.546   -0.52   0.8315      0.803  \n",
            "4             515.0   6031.0    4.438    0.07   1.0950      1.046  \n",
            "5              95.0   6046.0    4.486   -0.08   1.0530      0.972  \n",
            "First 5 rows of y:\n",
            " 0    1\n",
            "1    1\n",
            "3    0\n",
            "4    1\n",
            "5    1\n",
            "Name: koi_disposition, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1b48867"
      },
      "source": [
        "## Define Preprocessing and Model Pipelines\n",
        "\n",
        "### Subtask:\n",
        "Automatically detect numeric features. Create a `ColumnTransformer` for numeric features that includes median imputation and `StandardScaler`. Then, for each model (SVM, RandomForest, AdaBoost, MLPClassifier), construct an `sklearn.pipeline.Pipeline` that first applies the `ColumnTransformer`, then an optional `PCA` step (which can be `PCA(n_components=0.95)` or `passthrough`), and finally the respective model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "423e3791"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining preprocessing and model pipelines. I will start by importing all necessary libraries and then proceed with identifying numeric features, building the `ColumnTransformer`, and finally constructing individual pipelines for each classification model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "debdd1ec",
        "outputId": "971690b8-1072-4966-97b0-cc652a80c51f"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Identify numeric features\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Create a numeric transformer for imputation and scaling\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Create a ColumnTransformer to apply transformations to numeric features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features)\n",
        "    ],\n",
        "    remainder='passthrough' # Keep other columns (if any) as they are\n",
        ")\n",
        "\n",
        "# Define pipelines for each model\n",
        "\n",
        "# SVM Pipeline\n",
        "pipeline_svm = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('pca', PCA(random_state=42)),\n",
        "    ('classifier', SVC(random_state=42, class_weight='balanced'))\n",
        "])\n",
        "\n",
        "# RandomForest Pipeline\n",
        "pipeline_rf = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('pca', PCA(random_state=42)),\n",
        "    ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
        "])\n",
        "\n",
        "# AdaBoost Pipeline\n",
        "pipeline_ada = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('pca', PCA(random_state=42)),\n",
        "    ('classifier', AdaBoostClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# MLPClassifier Pipeline\n",
        "pipeline_mlp = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('pca', PCA(random_state=42)),\n",
        "    ('classifier', MLPClassifier(random_state=42, max_iter=1000))\n",
        "])\n",
        "\n",
        "print(\"Defined Pipelines:\")\n",
        "print(\"\\nSVM Pipeline:\", pipeline_svm)\n",
        "print(\"\\nRandomForest Pipeline:\", pipeline_rf)\n",
        "print(\"\\nAdaBoost Pipeline:\", pipeline_ada)\n",
        "print(\"\\nMLPClassifier Pipeline:\", pipeline_mlp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined Pipelines:\n",
            "\n",
            "SVM Pipeline: Pipeline(steps=[('preprocessor',\n",
            "                 ColumnTransformer(remainder='passthrough',\n",
            "                                   transformers=[('num',\n",
            "                                                  Pipeline(steps=[('imputer',\n",
            "                                                                   SimpleImputer(strategy='median')),\n",
            "                                                                  ('scaler',\n",
            "                                                                   StandardScaler())]),\n",
            "                                                  Index(['koi_period', 'koi_duration', 'koi_depth', 'koi_impact',\n",
            "       'koi_model_snr', 'koi_num_transits', 'st_teff', 'st_logg', 'st_met',\n",
            "       'st_mass', 'st_radius'],\n",
            "      dtype='object'))])),\n",
            "                ('pca', PCA(random_state=42)),\n",
            "                ('classifier', SVC(class_weight='balanced', random_state=42))])\n",
            "\n",
            "RandomForest Pipeline: Pipeline(steps=[('preprocessor',\n",
            "                 ColumnTransformer(remainder='passthrough',\n",
            "                                   transformers=[('num',\n",
            "                                                  Pipeline(steps=[('imputer',\n",
            "                                                                   SimpleImputer(strategy='median')),\n",
            "                                                                  ('scaler',\n",
            "                                                                   StandardScaler())]),\n",
            "                                                  Index(['koi_period', 'koi_duration', 'koi_depth', 'koi_impact',\n",
            "       'koi_model_snr', 'koi_num_transits', 'st_teff', 'st_logg', 'st_met',\n",
            "       'st_mass', 'st_radius'],\n",
            "      dtype='object'))])),\n",
            "                ('pca', PCA(random_state=42)),\n",
            "                ('classifier',\n",
            "                 RandomForestClassifier(class_weight='balanced',\n",
            "                                        random_state=42))])\n",
            "\n",
            "AdaBoost Pipeline: Pipeline(steps=[('preprocessor',\n",
            "                 ColumnTransformer(remainder='passthrough',\n",
            "                                   transformers=[('num',\n",
            "                                                  Pipeline(steps=[('imputer',\n",
            "                                                                   SimpleImputer(strategy='median')),\n",
            "                                                                  ('scaler',\n",
            "                                                                   StandardScaler())]),\n",
            "                                                  Index(['koi_period', 'koi_duration', 'koi_depth', 'koi_impact',\n",
            "       'koi_model_snr', 'koi_num_transits', 'st_teff', 'st_logg', 'st_met',\n",
            "       'st_mass', 'st_radius'],\n",
            "      dtype='object'))])),\n",
            "                ('pca', PCA(random_state=42)),\n",
            "                ('classifier', AdaBoostClassifier(random_state=42))])\n",
            "\n",
            "MLPClassifier Pipeline: Pipeline(steps=[('preprocessor',\n",
            "                 ColumnTransformer(remainder='passthrough',\n",
            "                                   transformers=[('num',\n",
            "                                                  Pipeline(steps=[('imputer',\n",
            "                                                                   SimpleImputer(strategy='median')),\n",
            "                                                                  ('scaler',\n",
            "                                                                   StandardScaler())]),\n",
            "                                                  Index(['koi_period', 'koi_duration', 'koi_depth', 'koi_impact',\n",
            "       'koi_model_snr', 'koi_num_transits', 'st_teff', 'st_logg', 'st_met',\n",
            "       'st_mass', 'st_radius'],\n",
            "      dtype='object'))])),\n",
            "                ('pca', PCA(random_state=42)),\n",
            "                ('classifier', MLPClassifier(max_iter=1000, random_state=42))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "485fbff7"
      },
      "source": [
        "## Configure GridSearchCV for Model Selection\n",
        "\n",
        "### Subtask:\n",
        "Define comprehensive hyperparameter grids for each model, ensuring computational feasibility as specified by the user. For instance, RandomForest: `n_estimators: [200, 400]`, `max_depth: [None, 10, 20]`, `min_samples_split: [2, 5]`; SVM: `C: [0.5, 1, 5]`, `gamma: ['scale', 0.01]`; AdaBoost: `n_estimators: [100, 200]`, `learning_rate: [0.5, 1.0]`; MLP: `hidden_layer_sizes: [(128,64), (256,128)]`, `alpha: [0.0001, 0.001]`. Also include `pca`: `[PCA(n_components=0.95), 'passthrough']` in the grids for all models. For SVM and RandomForest, set `class_weight='balanced'` in the model parameters. Initialize `GridSearchCV` for each model using `StratifiedKFold` (n_splits=5 or 10), and specify `scoring='f1'` while also configuring to compute `roc_auc`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33b3e200"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define parameter grids for each model, including PCA options, and then initialize `GridSearchCV` objects with `StratifiedKFold` and appropriate scoring metrics as specified in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5b0595b",
        "outputId": "5c1f4cef-c00a-4b4b-a74d-96995295dc5b"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Define parameter grids for each model\n",
        "\n",
        "# SVM Parameters\n",
        "param_grid_svm = {\n",
        "    'pca': [PCA(n_components=0.95, random_state=42), 'passthrough'],\n",
        "    'classifier__C': [0.5, 1, 5],\n",
        "    'classifier__gamma': ['scale', 0.01]\n",
        "}\n",
        "\n",
        "# RandomForest Parameters\n",
        "param_grid_rf = {\n",
        "    'pca': [PCA(n_components=0.95, random_state=42), 'passthrough'],\n",
        "    'classifier__n_estimators': [200, 400],\n",
        "    'classifier__max_depth': [None, 10, 20],\n",
        "    'classifier__min_samples_split': [2, 5]\n",
        "}\n",
        "\n",
        "# AdaBoost Parameters\n",
        "param_grid_ada = {\n",
        "    'pca': [PCA(n_components=0.95, random_state=42), 'passthrough'],\n",
        "    'classifier__n_estimators': [100, 200],\n",
        "    'classifier__learning_rate': [0.5, 1.0]\n",
        "}\n",
        "\n",
        "# MLPClassifier Parameters\n",
        "param_grid_mlp = {\n",
        "    'pca': [PCA(n_components=0.95, random_state=42), 'passthrough'],\n",
        "    'classifier__hidden_layer_sizes': [(128,64), (256,128)],\n",
        "    'classifier__alpha': [0.0001, 0.001]\n",
        "}\n",
        "\n",
        "# Initialize StratifiedKFold for cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV for each model\n",
        "grid_svm = GridSearchCV(pipeline_svm, param_grid_svm, cv=skf, scoring=['f1', 'roc_auc'], refit='f1', n_jobs=-1, verbose=2)\n",
        "grid_rf = GridSearchCV(pipeline_rf, param_grid_rf, cv=skf, scoring=['f1', 'roc_auc'], refit='f1', n_jobs=-1, verbose=2)\n",
        "grid_ada = GridSearchCV(pipeline_ada, param_grid_ada, cv=skf, scoring=['f1', 'roc_auc'], refit='f1', n_jobs=-1, verbose=2)\n",
        "grid_mlp = GridSearchCV(pipeline_mlp, param_grid_mlp, cv=skf, scoring=['f1', 'roc_auc'], refit='f1', n_jobs=-1, verbose=2)\n",
        "\n",
        "print(\"Hyperparameter grids and GridSearchCV objects initialized.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameter grids and GridSearchCV objects initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ba515f5"
      },
      "source": [
        "## Train Models and Evaluate Performance\n",
        "\n",
        "### Subtask:\n",
        "Execute `GridSearchCV` for each defined model. After completion, extract the `cv_results_` to evaluate model performance. Identify the best model by analyzing `mean_test_f1` and `std_test_f1` for each parameter combination, along with the `ROC-AUC` score. The selection criteria will prioritize models with high mean F1-score, low variance across folds (low `std_test_f1`), and strong ROC-AUC, rather than solely relying on the highest `best_score_`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19503e01"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to execute the `GridSearchCV` for each model defined earlier, then process and display their cross-validation results to evaluate performance. This involves fitting each grid search object, extracting the results, converting them to DataFrames, sorting by F1-score, and printing the top candidates.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8654b0fe",
        "outputId": "e846e666-4fa4-481d-b35b-fea82d839b7b"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "models = {\n",
        "    \"SVM\": grid_svm,\n",
        "    \"RandomForest\": grid_rf,\n",
        "    \"AdaBoost\": grid_ada,\n",
        "    \"MLPClassifier\": grid_mlp\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, grid_search in models.items():\n",
        "    print(f\"\\nFitting GridSearchCV for {name}...\")\n",
        "    grid_search.fit(X, y)\n",
        "    results[name] = pd.DataFrame(grid_search.cv_results_)\n",
        "    print(f\"Fitting for {name} complete.\")\n",
        "\n",
        "print(\"\\n--- Model Evaluation Results ---\")\n",
        "\n",
        "best_models_summary = []\n",
        "\n",
        "for name, df_results in results.items():\n",
        "    # Sort by mean_test_f1 to identify best performing configurations\n",
        "    df_sorted = df_results.sort_values(by='mean_test_f1', ascending=False)\n",
        "\n",
        "    # Select relevant columns for display\n",
        "    display_cols = ['params', 'mean_test_f1', 'std_test_f1', 'mean_test_roc_auc']\n",
        "\n",
        "    print(f\"\\nTop 5 results for {name}:\")\n",
        "    print(df_sorted[display_cols].head())\n",
        "\n",
        "    # Store the best configuration for overall comparison\n",
        "    best_f1_row = df_sorted.iloc[0]\n",
        "    best_models_summary.append({\n",
        "        'model': name,\n",
        "        'mean_f1': best_f1_row['mean_test_f1'],\n",
        "        'std_f1': best_f1_row['std_test_f1'],\n",
        "        'mean_roc_auc': best_f1_row['mean_test_roc_auc'],\n",
        "        'params': best_f1_row['params']\n",
        "    })\n",
        "\n",
        "# Convert summary to DataFrame for easier overall comparison\n",
        "summary_df = pd.DataFrame(best_models_summary)\n",
        "\n",
        "print(\"\\n--- Overall Best Model Candidates (based on highest mean_test_f1) ---\")\n",
        "print(summary_df.sort_values(by='mean_f1', ascending=False))\n",
        "\n",
        "# Manual selection of the best model based on F1, std F1, and ROC-AUC\n",
        "# This step requires looking at the printed results and making an informed decision.\n",
        "# For automated selection, we can choose the one with the highest mean_f1 as a primary, then lowest std_f1 as a secondary, and highest roc_auc as tertiary.\n",
        "\n",
        "# To select the best model, we'll sort by mean_f1 (desc), then by std_f1 (asc), then by mean_roc_auc (desc)\n",
        "overall_best_model_info = summary_df.sort_values(by=['mean_f1', 'std_f1', 'mean_roc_auc'], ascending=[False, True, False]).iloc[0]\n",
        "\n",
        "print(\"\\n\\n--- Selected Best Model ---\")\n",
        "print(f\"Model: {overall_best_model_info['model']}\")\n",
        "print(f\"Mean F1-score: {overall_best_model_info['mean_f1']:.4f}\")\n",
        "print(f\"Std F1-score: {overall_best_model_info['std_f1']:.4f}\")\n",
        "print(f\"Mean ROC-AUC: {overall_best_model_info['mean_roc_auc']:.4f}\")\n",
        "print(f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fitting GridSearchCV for SVM...\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Fitting for SVM complete.\n",
            "\n",
            "Fitting GridSearchCV for RandomForest...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "Fitting for RandomForest complete.\n",
            "\n",
            "Fitting GridSearchCV for AdaBoost...\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "Fitting for AdaBoost complete.\n",
            "\n",
            "Fitting GridSearchCV for MLPClassifier...\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "Fitting for MLPClassifier complete.\n",
            "\n",
            "--- Model Evaluation Results ---\n",
            "\n",
            "Top 5 results for SVM:\n",
            "                                              params  mean_test_f1  \\\n",
            "9  {'classifier__C': 5, 'classifier__gamma': 'sca...      0.828904   \n",
            "8  {'classifier__C': 5, 'classifier__gamma': 'sca...      0.828228   \n",
            "5  {'classifier__C': 1, 'classifier__gamma': 'sca...      0.811323   \n",
            "4  {'classifier__C': 1, 'classifier__gamma': 'sca...      0.810944   \n",
            "0  {'classifier__C': 0.5, 'classifier__gamma': 's...      0.804215   \n",
            "\n",
            "   std_test_f1  mean_test_roc_auc  \n",
            "9     0.010614           0.940397  \n",
            "8     0.009596           0.940015  \n",
            "5     0.008197           0.929517  \n",
            "4     0.008744           0.929292  \n",
            "0     0.006708           0.923635  \n",
            "\n",
            "Top 5 results for RandomForest:\n",
            "                                               params  mean_test_f1  \\\n",
            "23  {'classifier__max_depth': 20, 'classifier__min...      0.893831   \n",
            "17  {'classifier__max_depth': 20, 'classifier__min...      0.893582   \n",
            "19  {'classifier__max_depth': 20, 'classifier__min...      0.893294   \n",
            "5   {'classifier__max_depth': None, 'classifier__m...      0.893007   \n",
            "1   {'classifier__max_depth': None, 'classifier__m...      0.892948   \n",
            "\n",
            "    std_test_f1  mean_test_roc_auc  \n",
            "23     0.008377           0.973300  \n",
            "17     0.006387           0.972823  \n",
            "19     0.006575           0.973045  \n",
            "5      0.008835           0.973246  \n",
            "1      0.005862           0.973580  \n",
            "\n",
            "Top 5 results for AdaBoost:\n",
            "                                              params  mean_test_f1  \\\n",
            "7  {'classifier__learning_rate': 1.0, 'classifier...      0.853620   \n",
            "5  {'classifier__learning_rate': 1.0, 'classifier...      0.841003   \n",
            "3  {'classifier__learning_rate': 0.5, 'classifier...      0.839341   \n",
            "1  {'classifier__learning_rate': 0.5, 'classifier...      0.827819   \n",
            "6  {'classifier__learning_rate': 1.0, 'classifier...      0.745004   \n",
            "\n",
            "   std_test_f1  mean_test_roc_auc  \n",
            "7     0.008080           0.954114  \n",
            "5     0.006722           0.946997  \n",
            "3     0.008261           0.945735  \n",
            "1     0.007801           0.938298  \n",
            "6     0.009749           0.886060  \n",
            "\n",
            "Top 5 results for MLPClassifier:\n",
            "                                              params  mean_test_f1  \\\n",
            "5  {'classifier__alpha': 0.001, 'classifier__hidd...      0.863257   \n",
            "1  {'classifier__alpha': 0.0001, 'classifier__hid...      0.862326   \n",
            "6  {'classifier__alpha': 0.001, 'classifier__hidd...      0.861250   \n",
            "3  {'classifier__alpha': 0.0001, 'classifier__hid...      0.860975   \n",
            "2  {'classifier__alpha': 0.0001, 'classifier__hid...      0.860471   \n",
            "\n",
            "   std_test_f1  mean_test_roc_auc  \n",
            "5     0.007846           0.956378  \n",
            "1     0.007689           0.957085  \n",
            "6     0.009771           0.959514  \n",
            "3     0.012304           0.958280  \n",
            "2     0.012117           0.960615  \n",
            "\n",
            "--- Overall Best Model Candidates (based on highest mean_test_f1) ---\n",
            "           model   mean_f1    std_f1  mean_roc_auc  \\\n",
            "1   RandomForest  0.893831  0.008377      0.973300   \n",
            "3  MLPClassifier  0.863257  0.007846      0.956378   \n",
            "2       AdaBoost  0.853620  0.008080      0.954114   \n",
            "0            SVM  0.828904  0.010614      0.940397   \n",
            "\n",
            "                                              params  \n",
            "1  {'classifier__max_depth': 20, 'classifier__min...  \n",
            "3  {'classifier__alpha': 0.001, 'classifier__hidd...  \n",
            "2  {'classifier__learning_rate': 1.0, 'classifier...  \n",
            "0  {'classifier__C': 5, 'classifier__gamma': 'sca...  \n",
            "\n",
            "\n",
            "--- Selected Best Model ---\n",
            "Model: RandomForest\n",
            "Mean F1-score: 0.8938\n",
            "Std F1-score: 0.0084\n",
            "Mean ROC-AUC: 0.9733\n",
            "Parameters: {'classifier__max_depth': 20, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 400, 'pca': 'passthrough'}\n",
            "\n",
            "Best overall pipeline (`best_pipeline`) from RandomForest has been stored.\n",
            "Best overall parameters (`best_params`) from RandomForest has been stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e633afbb"
      },
      "source": [
        "## Retrain Best Model and Save Pipeline\n",
        "\n",
        "### Subtask:\n",
        "Based on the comprehensive evaluation in the previous step, select the overall best model and its optimal hyperparameters. Retrain this entire best `sklearn.pipeline.Pipeline` (which includes the `ColumnTransformer`, the chosen PCA configuration, and the best model) on the *full training dataset* (`X` and `y`). Finally, save this production-ready pipeline as 'classification_pipeline.pkl' using `joblib` for deployment, ensuring it can accept new raw input data for consistent preprocessing and prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13f34c4d"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to import the `joblib` library, retrain the `best_pipeline` on the full dataset, and then save the retrained pipeline to a file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37a0827a",
        "outputId": "37b1d6df-a4d8-45a0-e6b2-3d0ef2825fa7"
      },
      "source": [
        "import joblib\n",
        "\n",
        "# Retrain the best_pipeline on the full dataset (X and y)\n",
        "print(\"Retraining the best pipeline on the full dataset...\")\n",
        "\n",
        "best_pipeline.fit(X, y)\n",
        "print(\"Retraining complete.\")\n",
        "\n",
        "# Save the retrained pipeline\n",
        "joblib.dump(best_pipeline, 'classification_pipeline.pkl')\n",
        "print(\"Saved the retrained pipeline as 'classification_pipeline.pkl'.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retraining the best pipeline on the full dataset...\n",
            "Retraining complete.\n",
            "Saved the retrained pipeline as 'classification_pipeline.pkl'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ff2e7bc"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Present the final metrics of the chosen best model, along with the complete, well-commented, and modular Python code for the classification system. Ensure the code is ready for deployment and the serialized pipeline can accept new raw data for predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82268d04"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The final metrics of the chosen best model are:\n",
        "*   **Model:** RandomForest\n",
        "*   **Mean F1-score:** 0.8997\n",
        "*   **Standard Deviation of F1-score:** 0.0108\n",
        "*   **Mean ROC-AUC:** 0.9769\n",
        "\n",
        "*   **Best Parameters:** `{'classifier__max_depth': None, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 400, 'pca': 'passthrough'}`\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The initial dataset, after removing 'CANDIDATE' rows and mapping 'CONFIRMED' to 1 and 'FALSE POSITIVE' to 0, resulted in a dataset of 7585 observations with 24 features for modeling.\n",
        "*   A robust preprocessing pipeline was established, including median imputation for missing values and `StandardScaler` for numeric features, integrated into `ColumnTransformer`.\n",
        "*   Four distinct classification models (SVM, RandomForest, AdaBoost, and MLPClassifier) were evaluated using `GridSearchCV` with `StratifiedKFold` (n\\_splits=5), optimizing for `f1` score while also computing `roc_auc`. Each model pipeline incorporated the preprocessing steps and an optional `PCA` component.\n",
        "*   Among the evaluated models, the **RandomForestClassifier** exhibited the best overall performance, achieving a mean F1-score of 0.8997\n",
        "*   The best-performing RandomForest configuration utilized `n_estimators=400`, `max_depth=None`, `min_samples_split=5`, and opted for `pca='passthrough'`, indicating that PCA might not have been beneficial for this specific model configuration.\n",
        "*   Other models achieved lower mean F1-scores: SVM (0.8456), AdaBoost (0.8775), and MLPClassifier (0.8623).\n",
        "*   The selected best RandomForest pipeline, including all preprocessing steps, was retrained on the full dataset and successfully saved as 'classification\\_pipeline.pkl', making it ready for deployment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(X.columns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yQ0LrG84H8Q",
        "outputId": "08a1a6c0-21ef-40d3-d463-c4f4c035dd8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['koi_period', 'koi_duration', 'koi_depth', 'koi_impact', 'koi_model_snr', 'koi_num_transits', 'st_teff', 'st_logg', 'st_met', 'st_mass', 'st_radius']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "output list of columns :\n",
        "['koi_period', 'koi_duration', 'koi_depth', 'koi_impact', 'koi_model_snr', 'koi_num_transits', 'koi_ror', 'st_teff', 'st_logg', 'st_met', 'st_mass', 'st_radius', 'st_dens', 'teff_err1', 'teff_err2', 'logg_err1', 'logg_err2', 'feh_err1', 'feh_err2', 'mass_err1', 'mass_err2', 'radius_err1', 'radius_err2']\n"
      ],
      "metadata": {
        "id": "5gaGk6hgSsMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "E8AYP_ht4xjf",
        "outputId": "d4f6d9b7-2ff3-40a7-91d9-6b7aebe7695e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'best_pipeline' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2907320541.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'best_pipeline' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RKmPGjwikbOY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}